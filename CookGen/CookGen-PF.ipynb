{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv0NhY3wjqRs",
        "outputId": "5e53a123-8bae-4c63-803b-b72f752c5f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mount google drive to load data\n",
        "from google.colab import drive\n",
        "\n",
        "#mount google drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "#store path to files\n",
        "path = \"gdrive/MyDrive/AGNNs/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read in the data into data file\n",
        "data = path+\"annotations_combined_v2.csv\"\n",
        "f = open(data,'r') #file pointer\n",
        "data_items = f.read().splitlines()[1:] #read all file lines separated by '\\n'"
      ],
      "metadata": {
        "id": "8vkx6LT9kOpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample n random data items\n",
        "n = len(data_items)\n",
        "\n",
        "from random import sample \n",
        "small_data = sample(data_items,n)\n",
        "len(small_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vImzrbA7nwoq",
        "outputId": "f8a3f5b6-6f56-4c33-aee6-2d51e34a2dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8898"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#place holder lists for formatted data\n",
        "formatted_X, formatted_Y = [], []\n",
        "\n",
        "#parse data and extract instructions and action pair\n",
        "for data_item in small_data:\n",
        "\n",
        "  instr = data_item.split(',')[1:-1]; formatted_X += instr #instruction \n",
        "  action = data_item.split(',')[-1]; formatted_Y += [action] #cooking action"
      ],
      "metadata": {
        "id": "pRigZ-oro4Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install library to use BERT's tokenizer\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "72B3Gqeip4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get BERT's tokenizer\n",
        "from transformers import BertTokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "Vb3DKbc9qEFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get all tokens in the dataset\n",
        "all_tokens = set() #place holder set for unique tokens\n",
        "for data_item in small_data:\n",
        "  #add tokens to set\n",
        "  [all_tokens.add(token) for token in tokenizer.tokenize(data_item)]\n",
        "all_tokens = list(all_tokens) #convert to list for indexing"
      ],
      "metadata": {
        "id": "O5OySdXJqcCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrapper for encoding and decoding texts\n",
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               tokens = None):\n",
        "    \n",
        "    #set tokens to the provided token set\n",
        "    self.tokens = tokens\n",
        "\n",
        "    #store SEP token from BERT's tokenizer as end of program marker\n",
        "    self.EOS = tokenizer.decode(tokenizer.encode('dummy'))[-1]; self.tokens += [self.EOS]\n",
        "\n",
        "    #store number of tokens\n",
        "    self.vocab_size = len(self.tokens)\n",
        "\n",
        "  def encode(self,\n",
        "             text):\n",
        "    \n",
        "    '''\n",
        "    this function encodes the text\n",
        "    using the token set provided at instantiation\n",
        "    '''\n",
        "\n",
        "    tokens = tokenizer.tokenize(text) #BERT tokenize\n",
        "    #get token indices from wrapper's token list\n",
        "    encodings = [self.tokens.index(token) for token in tokens]\n",
        "\n",
        "    #return token encodings\n",
        "    return encodings\n",
        "\n",
        "  def decode(self,\n",
        "             encodings):\n",
        "    '''this function decodes the encodings back\n",
        "       into the text\n",
        "    '''\n",
        "\n",
        "    #decode by looking up the wrapper's token list\n",
        "    decodings = [self.tokens[encoding] for encoding in encodings]\n",
        "\n",
        "    #return decoded string\n",
        "    return ' '.join(decodings)"
      ],
      "metadata": {
        "id": "2xuk132JqSl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create custom tokenizer\n",
        "c_tokenizer = Tokenizer(tokens = all_tokens)"
      ],
      "metadata": {
        "id": "Bh63oLeauO_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from random import sample, shuffle\n",
        "\n",
        "#implement data loader class which will provide utility functions for\n",
        "#1. Random batch sampling\n",
        "#2. Providing encoded and decoded text with EOS markers\n",
        "class Dataloader(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               data_tokenizer = None, #tokenizer used to encode and decode\n",
        "               data_X = None, #X in the format of formatted_X\n",
        "               data_Y = None, #Y in the format of formatted_Y\n",
        "               context_size = 1000): #max context size\n",
        "\n",
        "    self.data = list(zip(data_X, data_Y)) #creat (x,y) pair list\n",
        "    self.n_data = len(data) #store no. of (x,y) pairs\n",
        "    self.tokenizer = data_tokenizer #e.g., c_tokenizer or BERT's tokenizer\n",
        "    self.context_size = context_size #default 1000\n",
        "\n",
        "  def get_batch(self,\n",
        "                n = None): #if n == None, whole dataset will be returned\n",
        "\n",
        "    batch = self.data #initalize batch to whole dataset\n",
        "    \n",
        "    if n == None: #if no batch size specified\n",
        "      shuffle(batch) #shuffle batch to remove autocorrelation      \n",
        "\n",
        "    else: #if batch size of n specified\n",
        "      batch = sample(self.data,n)\n",
        "\n",
        "    #construct contextualized data list\n",
        "    contextualized_data = [] #initialize place holder list\n",
        "    for data_item in batch:\n",
        "\n",
        "      x_item, y_item = data_item[0], data_item[1] #get (x,y) pair\n",
        "      x_encodings = self.tokenizer.encode(x_item) #encode x\n",
        "      y_encodings = self.tokenizer.encode(y_item) #encode y\n",
        "      eos_encoding = [self.tokenizer.tokens.index(self.tokenizer.EOS)] #EOS marker\n",
        "      #get merged encodings\n",
        "      merged_encodings = x_encodings + y_encodings + eos_encoding\n",
        "      n_merged_encodings = len(merged_encodings) #get no. of encodings\n",
        "      for t in range(1,n_merged_encodings): #t is index into list of merged encodings\n",
        "        contextualized_data.append([merged_encodings[:t][-self.context_size:],[merged_encodings[t]]])\n",
        "      \n",
        "    return contextualized_data"
      ],
      "metadata": {
        "id": "Uwei2GNewXXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataloader with the custom tokenizer and formatted data\n",
        "dl = Dataloader(data_tokenizer = c_tokenizer,\n",
        "                data_X = formatted_X,\n",
        "                data_Y = formatted_Y)"
      ],
      "metadata": {
        "id": "4n6eP-HY00Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get device e.g., GPU, TPU, CPU\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "P1u9QlYC3f1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn #import pytorch's neural net module\n",
        "import torch.nn.functional as F #import pytorch's activation functions\n",
        "from tqdm import tqdm #import progress bar\n",
        "\n",
        "class generator(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size = None, #vocabulary size, i.e., no. of tokens\n",
        "               emb_size = None, #size of token embeddings\n",
        "               context_size = 1000, #max context size\n",
        "               order = 3): #max polynomial order\n",
        "    \n",
        "    super().__init__() #call superclass constructor\n",
        "\n",
        "    self.vocab_size = vocab_size \n",
        "    self.emb_size = emb_size \n",
        "    self.context_size = context_size \n",
        "    self.order = order \n",
        "\n",
        "    #initialize params\n",
        "    self.embeddings = nn.Embedding(self.vocab_size, self.emb_size) #embedding layer\n",
        "    self.pos_embeddings = nn.Embedding(self.context_size, self.emb_size) #positional encodings\n",
        "    self.head = nn.Linear(self.emb_size, self.vocab_size) #classification head\n",
        "\n",
        "  def normed(self,\n",
        "             T):\n",
        "    '''function to normalize\n",
        "       the tensor M\n",
        "    '''\n",
        "    \n",
        "    #get norm of the tensor\n",
        "    norm = torch.linalg.norm(T)\n",
        "\n",
        "    #divide by norm\n",
        "    return (torch.div(T,norm.item()))\n",
        "\n",
        "  def forward(self,\n",
        "              token_encodings):\n",
        "    \n",
        "    n_tokens = len(token_encodings) #input size\n",
        "    token_encodings = torch.tensor(token_encodings) #convert to pytorch tensor\n",
        "    token_encodings.to(device) #place tensor on compute device\n",
        "    token_embeddings = self.embeddings(token_encodings) #pass through embedding layer\n",
        "    pos_embeddings = self.pos_embeddings(torch.arange(n_tokens)) #pass through position embedding layer\n",
        "    token_embeddings += pos_embeddings #add token and position embeddings\n",
        "\n",
        "    #compute polynomial averages of upto order k, norm to standardize column ranges\n",
        "    avgs = torch.row_stack([self.normed(torch.mean(torch.pow(token_embeddings,k),dim=0)) for k in range(self.order)])\n",
        "    logits = self.head(avgs)[-1] #extract logits\n",
        "    return logits #return extracted logits\n",
        "\n",
        "  def train(self,\n",
        "            dl,\n",
        "            batch_size = None,\n",
        "            epochs = 1000):\n",
        "    \n",
        "    '''trains the generator function\n",
        "       dl: dataloader object that allows tokenizer and data access\n",
        "    '''\n",
        "\n",
        "    optimizer = torch.optim.AdamW(self.parameters()) #use AdamW optimizer\n",
        "\n",
        "    #training loop\n",
        "    for i in tqdm(range(epochs)):\n",
        "\n",
        "      batch = (dl.get_batch(n=batch_size) if batch_size != None else dl.get_batch()) #get random batch\n",
        "      n_batch = len(batch) #store batch size\n",
        "      loss = F.cross_entropy #initialize loss function for multilabel classification\n",
        "\n",
        "      batch_loss = 0.0 #initialize average batch loss to zero\n",
        "      for item in batch: #compute average batch loss \n",
        "        x, y = item[0], item[1][0] #get (x, y) pair\n",
        "        logits = self(x) #forward pass\n",
        "        #one-hot encode the target y\n",
        "        targets = [0.0]*self.vocab_size; targets[y] = 1.0\n",
        "        targets = torch.tensor(targets) #convert to pytorch tensor\n",
        "        batch_loss += loss(logits,targets) #add to batch loss\n",
        "\n",
        "      batch_loss /= n_batch #obtain averaged batch loss\n",
        "      print (batch_loss.item()) #print scalar part (item) of batch loss tensor\n",
        "      batch_loss.backward() #compute gradients\n",
        "      optimizer.step() #gradient descent step\n",
        "      optimizer.zero_grad() #zero gradients to remove accumulated gradients"
      ],
      "metadata": {
        "id": "khS066Yv39yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize generator object\n",
        "g = generator(vocab_size = c_tokenizer.vocab_size,\n",
        "              emb_size = 200)"
      ],
      "metadata": {
        "id": "QD2kMfTw8LAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the generator\n",
        "g.train(dl)"
      ],
      "metadata": {
        "id": "VYnkgaQB8fFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (sum(p.numel() for p in g.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PVk3VdYIVsJ",
        "outputId": "b1f9018e-e0eb-4fc8-bd99-1646976b6c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get five examples to test generation\n",
        "qual_eval = formatted_X[:5]\n",
        "\n",
        "outputs = []\n",
        "for item in qual_eval:\n",
        "\n",
        "  item_output = []\n",
        "\n",
        "  for g_idx in range(1):\n",
        "    encodings = dl.tokenizer.encode(item)\n",
        "    logits = g(encodings)\n",
        "    logits = F.softmax(logits,dim=-1)\n",
        "    next_id = torch.multinomial(logits,num_samples = 1)\n",
        "    item_output += next_id.tolist()\n",
        "  \n",
        "  outputs.append(dl.tokenizer.decode(item_output))\n",
        "\n",
        "print (\"test inputs: \",qual_eval)\n",
        "print (\"predicted outputs: \",outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aWEQBZ_DdLo",
        "outputId": "2b17e880-f69b-479d-b48b-114dc1a3e0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test inputs:  ['beat 2 min.', 'Season with salt and pepper to tasteprobably only needs a good grind of pepper to finish it off.', '\"season', 'Place turkey in an oven roasting bag.', 'Mix until blended.']\n",
            "predicted outputs:  ['beat', 'grind', 'place', 'mix', 'heat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"ground truth:\", formatted_Y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ImhuWB_FIsT",
        "outputId": "04515f38-6b14-4950-d91f-5cf1d5c5843c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ground truth: ['beat', 'grind\"', 'place', 'mix', 'heat']\n"
          ]
        }
      ]
    }
  ]
}